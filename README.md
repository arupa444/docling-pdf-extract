# The Extra Extractor RAG

<div align="center">

**A Python-based document extraction, conversion and execution toolkit for Retrieval-Augmented Generation (RAG) workflows**

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Backend-success.svg)](https://fastapi.tiangolo.com/)
[![Gemini](https://img.shields.io/badge/Gemini-API-blue.svg)]()
[![RAG](https://img.shields.io/badge/RAG-Retrieval--Augmented-purple.svg)]()
[![LangChain](https://img.shields.io/badge/LangChain-Framework-blueviolet.svg)]()
[![Docling](https://img.shields.io/badge/Docling-Document%20Extraction-orange.svg)]()
[![FAISS](https://img.shields.io/badge/VectorDB-FAISS-orange.svg)]()
[![Playwright](https://img.shields.io/badge/Playwright-JS%20Rendering-brightgreen.svg)]()
[![Scrapy](https://img.shields.io/badge/Scrapy-Web%20Crawler-red.svg)]()
[![OCR](https://img.shields.io/badge/OCR-Document%20Parsing-yellow.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

---

## üìñ Overview

**The Extra Extractor RAG** is a comprehensive toolkit designed to streamline the preprocessing pipeline for RAG applications. It converts raw web content (HTML, PDFs, websites) into clean, structured Markdown format that's optimized for embedding, chunking, and retrieval in RAG systems.

This project addresses a critical challenge in RAG development: transforming diverse document formats into consistent, high-quality text that can be efficiently processed by language models and vector databases.

### Why This Toolkit?

- üéØ **Clean Data = Better RAG**: LLMs perform significantly better with structured, clean text
- üéØ **Web Scrap = Data extraction**: Scrap web pages and websites(nested pages, specific to the domain)
- üîÑ **Unified Format**: Converts multiple formats (HTML, PDF, web pages) to Markdown
- ‚ö° **Production-Ready**: Modular architecture with REST API endpoints
- üõ†Ô∏è **Flexible**: Use individual scripts or integrate the full API
- üìä **RAG-Optimized**: Output is pre-formatted for chunking and embedding

---

## ‚ú® Key Features

### Document Conversion
- **HTML to Markdown**: Extract clean text from HTML pages while preserving structure
- **PDF to Markdown**: Multiple PDF extraction methods (PyMuPDF, standard)
- **Website Scraping**: Full website data extraction with crawler support
- **Batch Processing**: Convert multiple files efficiently

### RAG Pipeline Support
- Clean, structured output optimized for text chunking
- Preserves document hierarchy and formatting
- Removes noise and irrelevant content
- Ready for embedding generation
- Compatible with vector databases (FAISS, Pinecone, Weaviate, etc.)

### Flexible Architecture
- Standalone Python scripts for individual tasks
- REST API (`app.py`) for programmatic access
- Utility modules for custom workflows
- Configuration management

---

## üèóÔ∏è Repository Structure

```
The-Extra-Extractor-RAG/
‚îú‚îÄ‚îÄ .idea/                          # IDE configuration
‚îú‚îÄ‚îÄ config/                         # Configuration files
‚îú‚îÄ‚îÄ utils/                          # General utility functions
‚îú‚îÄ‚îÄ utilsForRAG/                    # RAG-specific utilities
‚îú‚îÄ‚îÄ HTMLs_PDFs_to_MD.py            # Combined HTML & PDF converter
‚îú‚îÄ‚îÄ htmlToMD.py                    # HTML to Markdown converter
‚îú‚îÄ‚îÄ pdfToMD.py                     # PDF to Markdown converter
‚îú‚îÄ‚îÄ pdf_to_md_pymupdf.py           # PyMuPDF-based PDF converter
‚îú‚îÄ‚îÄ websiteDataExtraction.py       # Website scraper & extractor
‚îú‚îÄ‚îÄ anythingButJSOrSPA.py          # Handler for non-JS/SPA sites
‚îú‚îÄ‚îÄ run_spidy.py                   # Web crawler runner
‚îú‚îÄ‚îÄ app.py                         # REST API server (endpoints)
‚îú‚îÄ‚îÄ try.py                         # Testing/experimentation script
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üöÄ Getting Started

### Prerequisites

- Python 3.12
- pip package manager and if you are using UV then thats better
- Git

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/arupa444/The-Extra-Extractor-RAG.git
cd The-Extra-Extractor-RAG
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Verify installation:**
```bash
python app.py --help
```

---

## üìö Usage Guide

### REST API Server

The `app.py` file provides REST API endpoints for programmatic access to all extraction features.

#### Starting the Server

```bash
uvicorn app:app --reload --port 8000
```

The server will start on `http://localhost:8000` (or configured port).

#### API Endpoints

##### 1. PDF to Markdown Conversion

##### Endpoints name...

- /OCR_On_Single_Upload
- /OCR_On_Folder_Or_Multiple_file_Uploads

```http
POST /api/v1/pdf-to-markdown
Content-Type: multipart/form-data



Parameters:
- file/folder: PDF files (multipart/form-data)
- subDir: (optional) Name the directory

Response:
{
  "markdown": "# Extracted content...",
  "SavedLocation": "# Saved location... "
}
```

##### also do other files like html and more

```http
POST /api/v1/html-to-markdown
Content-Type: application/json

Body:
{
  "file/folder": files, #html file and more
  "subDir": (optional) Name the directory
}

Response:
{
  "markdown": "# Extracted content...",
  "SavedLocation": "# Saved location... "
}
```

##### 2. Website Extraction

##### Endpoints name...

- /OCR_On_nonJS_nonSPA_Website
- /Multiple_OCRs_On_nonJS_nonSPA_Website
- /OCR_On_JS_SPA_Website
- /Multiple_OCRs_On_JS_SPA_Websites

```http
POST /api/v1/extract-website
Content-Type: application/json

Body:
{
  "webLink": "https://example.com", # if you want to extract a SPA and only one page
  "weblinks": ["https://example.com", "https://example1.com",.....] # multiple link that you want to fetch
  "subDir": (optional) Name the directory
}

Response:
{
    "markdown_content": "#markdown_content content" # only when there is only one SPA to extract
    "results": [     # multiple SPA
        {
            "filename": file.filename,
            "status": "success",
            "markdown_content": markdown_content
        },
        {
            "filename": file.filename,
            "status": "success",
            "markdown_content": markdown_content
        },.....
        ]
}
```

##### 3. RAG Processing

##### Endpoints name...

- RAG_On_Single_Upload
- RAG_On_Folder_Or_Multiple_file_Uploads
- RAG_On_nonJS_nonSPA_Website
- RAG_On_Multiple_nonJS_nonSPA_Website
- RAG_On_JS_SPA_Website
- RAG_On_Multiple_JS_SPA_Websites

```http
POST /api/v1/links-docs-websites-to-rag-embedding
Content-Type: application/json

Body:
{
    "file/files/webLink/webLinks": uploaded_file/uploaded_files/webLink/webLinks, # this depends on the endpoint you are using... and For JS build website... there is an another endpoint and for non JS based we have an different...
    "query": "...", #a question for the rag itself.... 
    "subDir": "..." # name of the dir
}

Response:
{
    "Top Result": "... (Score: ......)", # title of the chunk and the score
    "Final Answer": "...", # the answer of your query
    "markdown_content": "..." # the markdown content
}
# And it will also saves the data extracted from the web, docs and more... and also saved the embedding, chunks, propositions and more to that the faiss and an ID file to detect the ID.
```

##### 4. Main RAG endpoints (end-to-end website extraction).... 

##### Endpoint name...

- full_website_extraction
- full_website_extraction_and_conversation
- full_website_extraction_conversation_and_execution

```http
POST /api/v1/full_website_extraction
Content-Type: application/json

Body:
{
    "query": "...", # a question for the rag itself.... 
    "webLink": webLink, # Web link that you want to scrap fully... like the sub links and all
}

Response:
{
    "Top Result": "... (Score: ......)", # title of the chunk and the score
    "Final Answer": "...", # the answer of your query
    "SavedLocation": "..." # The saved location in the name of the website
}
# And it will also saves the data extracted from the web, docs and more... and also saved the embedding, chunks, propositions and more to that the faiss and an ID file to detect the ID.
```

### Command-Line Usage

#### Convert PDF to Markdown
```bash
# Using standard PDF converter
python pdfToMD.py input.pdf --output output.md

# Using PyMuPDF converter (better quality)
python pdf_to_md_pymupdf.py input.pdf --output output.md
```

#### Convert HTML to Markdown
```bash
# From URL
python htmlToMD.py https://example.com/article.html --output article.md

# From local file
python htmlToMD.py local_file.html --output output.md
```

#### Batch Convert HTML & PDF Files
```bash
python HTMLs_PDFs_to_MD.py --input-dir ./documents --output-dir ./markdown
```

#### Extract Website Data
```bash
# Basic extraction
python websiteDataExtraction.py https://example.com --output ./output

# With depth control
python websiteDataExtraction.py https://example.com --depth 3 --max-pages 100
```

#### Run Web Crawler
```bash
python run_spidy.py --start-url https://example.com --depth 2
```

---

## üîß Configuration

### Environment Variables

Create a `.env` file in the root directory:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=5000
API_DEBUG=False

# Extraction Settings
MAX_FILE_SIZE=50MB
TIMEOUT_SECONDS=300
MAX_DEPTH=5

# Output Settings
OUTPUT_FORMAT=markdown
PRESERVE_LINKS=true
REMOVE_IMAGES=false

# Logging
LOG_LEVEL=INFO
LOG_FILE=extractor.log
```

### Configuration File (`config/settings.py`)

Customize extraction behavior:

```python
# PDF Extraction Settings
PDF_CONFIG = {
    'engine': 'pymupdf',  # or 'standard'
    'extract_images': False,
    'preserve_layout': True
}

# HTML Extraction Settings
HTML_CONFIG = {
    'remove_scripts': True,
    'remove_styles': True,
    'preserve_links': True
}

# Web Scraping Settings
SCRAPER_CONFIG = {
    'user_agent': 'ExtraExtractorBot/1.0',
    'respect_robots_txt': True,
    'delay_seconds': 1
}
```

---

## üîÑ Integration with RAG Pipelines

### Step-by-Step RAG Integration

#### 1. **Extract and Convert Documents**
```python
def run_spider_process(url: str):
    # This runs the separate python script
    # sys.executable ensures we use the same python environment (venv)
    subprocess.run([sys.executable, "run_spidy.py", url])
```

```python
def run_HTMLs_PDFs_to_MDFile_process(subDirName: str):

subprocess.run([sys.executable, "HTMLs_PDFs_to_MD.py", subDirName], check=True)
json_output_path = Path(f"storeCurlData/{subDirName}/{subDirName}.json")

# 3. Read the file and return the data
if json_output_path.exists():
    try:
        with open(json_output_path, "r", encoding="utf-8") as f:
            accumulated_results = json.load(f)
        return accumulated_results
    except json.JSONDecodeError:
        print("Error: The generated JSON file was corrupted.")
        return []
else:
    print(f"Error: Expected output file not found at {json_output_path}")
    return []
```

#### 2. **Chunk the Text (Agentic chunker)**
```python
class AgenticChunker: # using the Agentic Chunker
    ...
    ...
    ...
    def generate_propositions(self, text: str | dict | List[Any]) -> List[str]:
        if self.print_logging:
            console.print("[bold blue]Generating propositions...[/bold blue]")

        print("Started generating propositions...")

        PROMPT = ChatPromptTemplate.from_messages([
            ("system", """
            Decompose the text into distinct, atomic propositions. And strictly don't miss any information.
            Return strictly a JSON list of strings.
            """),
            ("user", "{text}")
        ])

        runnable = PROMPT | self.llm | StrOutputParser()
        raw_response = runnable.invoke({"text": text})
        cleaned_response = (
            raw_response
            .replace("```json", "")
            .replace("```", "")
            .strip()
        )

        print("Clean response: ",cleaned_response)

    ...
    ...
    ...
    
    def _llm_judge_chunk(self, proposition, candidate_chunk_ids) -> None | str:
        outline = ""
        for cid in candidate_chunk_ids:
            c = self.chunks[cid]
            outline += f"Chunk ID: {cid}\nSummary: {c['summary']}\n\n"

        PROMPT = ChatPromptTemplate.from_messages([
            ("system", """
            Decide if the proposition belongs to any chunk below.
            Return ONLY the chunk_id or "No chunks".
            No explanation.
            No extra text.
            """),
            ("user", "Chunks:\n{outline}\nProposition:\n{proposition}")
        ])

        response = (PROMPT | self.llm | StrOutputParser()).invoke({
            "outline": outline,
            "proposition": self._to_text(proposition)
        }).strip()

        return response if response in candidate_chunk_ids else None

```

#### 3. **Generate Embeddings**
```python
self.embedder = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004",
    google_api_key=api_key
)

# or

self.embedder = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
```

#### 4. **Store in Vector Database**
```python
def __init__(self, dim=768):
    self.dim = dim
    self.index = faiss.IndexFlatIP(dim)
    ...
    ...
    def add(self, chunk_id, embedding):
        vec = np.array([embedding]).astype("float32")
        faiss.normalize_L2(vec)
        self.index.add(vec)
        self.chunk_ids.append(chunk_id)
    ...
    ...
    def save_local(self, folder_path, filename_prefix):
        """Saves both the FAISS index and the ID mapping"""

        # 1. Save FAISS Binary
        index_path = os.path.join(folder_path, f"{filename_prefix}.faiss")
        faiss.write_index(self.index, index_path)
```

#### 5. **Query and Retrieve**
```python
    def search(self, query_embedding, k=3):
    vec = np.array([query_embedding]).astype("float32")
    faiss.normalize_L2(vec)

    scores, ids = self.index.search(vec, k)

    results = []
    for idx, i in enumerate(ids[0]):
        if i != -1:  # FAISS returns -1 if not enough neighbors found
            results.append((self.chunk_ids[i], float(scores[0][idx])))
    return results

class Retrieve:
    def retrieve(query, chunker, memory_index):
        print(f"\n[bold magenta]Searching for:[/bold magenta] '{query}'")

        # Embed the query
        query_embedding = chunker.embedder.embed_query(query)

        # Search Index
        results = memory_index.search(query_embedding)
        return ...
    
class Answer:
    def answer(query, retrieved_chunks, llm):
        evidence_text = "\n\n".join(
            f"SOURCE ID: {c['chunk_id']}\nEVIDENCE:\n" + "\n".join(f"- {p}" for p in c["evidence"])
            for c in retrieved_chunks
        )

        PROMPT = ChatPromptTemplate.from_messages([
            ("system", """
            Answer the user's question using ONLY the provided evidence. 
            Cite the SOURCE ID for every fact you use.
            If you cannot answer based on the evidence, say so.
            """),
            ("user", "Question: {query}\n\nEvidence:\n{evidence}")
        ])

        runnable = PROMPT | llm | StrOutputParser()
        return runnable.invoke({"query": query, "evidence": evidence_text})
    
```

---

## üß© Use Cases

### 1. **Enterprise Knowledge Base**
Extract internal documentation (PDFs, wikis) into a searchable RAG system for employee Q&A.

### 2. **Research Paper Analysis**
Convert academic papers to Markdown for semantic search and citation networks.

### 3. **Legal Document Processing**
Extract contracts and legal documents for compliance checking and clause retrieval.

### 4. **Customer Support**
Build a support bot by extracting product documentation and FAQs.

### 5. **Content Aggregation**
Crawl and extract blog posts, articles, and news for content analysis.

---

## üõ†Ô∏è Advanced Features

### Custom Extractors

Create custom extractors by extending the base classes:

```python
from utils.base_extractor import BaseExtractor

class CustomExtractor(BaseExtractor):
    def extract(self, content):
        # Your custom extraction logic
        return cleaned_content
```

### Batch Processing

Process multiple files efficiently:

```python
from utilsForRAG.batch_processor import BatchProcessor

processor = BatchProcessor(
    input_dir='./documents',
    output_dir='./markdown',
    workers=4
)
processor.process_all()
```

### Quality Control

Built-in validation and quality checks:

```python
from utilsForRAG.validators import MarkdownValidator

validator = MarkdownValidator()
is_valid, issues = validator.validate(markdown_content)
```

---

## üìä Performance Optimization

### Tips for Large-Scale Extraction

1. **Use PyMuPDF for PDFs**: Faster and more accurate than standard parsers
2. **Enable Batch Processing**: Process multiple files in parallel
3. **Configure Chunk Sizes**: Optimize based on your RAG model's context window
4. **Cache Results**: Store extracted Markdown to avoid re-processing
5. **Filter Content**: Remove unnecessary sections before embedding

### Benchmarks

| Document Type | Size | Extraction Time | Quality Score |
|---------------|------|-----------------|---------------|
| PDF (text)    | 10MB | ~5 seconds      | 95%           |
| PDF (scanned) | 10MB | ~30 seconds     | 85%           |
| HTML          | 1MB  | ~1 second       | 98%           |
| Website       | 100 pages | ~2 minutes  | 92%           |

---

## üêõ Troubleshooting

### Common Issues

**Issue: PDF extraction fails**
```bash
# Solution: Install system dependencies
sudo apt-get install poppler-utils  # Linux
brew install poppler                 # macOS
```

**Issue: Website scraping blocked**
```python
# Solution: Configure user agent and delays
SCRAPER_CONFIG = {
    'user_agent': 'Mozilla/5.0...',
    'delay_seconds': 2
}
```

**Issue: Memory errors with large files**
```python
# Solution: Process in chunks
from utilsForRAG.chunked_processor import process_large_file
process_large_file('large.pdf', chunk_size=10)
```

---

## ü§ù Contributing

We welcome contributions! Here's how to get started:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes**
4. **Add tests**: Ensure your changes are tested
5. **Commit your changes**: `git commit -m 'Add amazing feature'`
6. **Push to the branch**: `git push origin feature/amazing-feature`
7. **Open a Pull Request**

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Run linting
flake8 .
black .
```

### Code Style

- Follow PEP 8 guidelines
- Use type hints
- Write docstrings for all functions
- Add unit tests for new features

---

## üìã Requirements

Key dependencies (see `requirements.txt` for complete list):

- `Flask` - REST API framework
- `PyMuPDF` - PDF processing
- `beautifulsoup4` - HTML parsing
- `requests` - HTTP requests
- `markdown` - Markdown generation
- `langchain` - RAG utilities (optional)
- `sentence-transformers` - Embeddings (optional)

---

## üîí Security Considerations

- **File Upload Validation**: All uploaded files are validated for type and size
- **URL Sanitization**: URLs are sanitized to prevent SSRF attacks
- **Rate Limiting**: API endpoints include rate limiting
- **Timeout Protection**: Long-running operations have configurable timeouts

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- Inspired by the RAG community and best practices
- Built with modern Python tools and libraries
- Thanks to all contributors and users

---

## üìû Support & Contact

- **Issues**: [GitHub Issues](https://github.com/arupa444/The-Extra-Extractor-RAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/arupa444/The-Extra-Extractor-RAG/discussions)
- **Email**: Create an issue for support requests

---

## üó∫Ô∏è Roadmap

- [ ] Support for more document formats (DOCX, PPTX)
- [ ] Improved OCR for scanned documents
- [ ] Real-time streaming for large files
- [ ] Docker containerization
- [ ] Cloud deployment guides (AWS, GCP, Azure)
- [ ] Pre-built RAG pipeline examples
- [ ] GraphQL API support
- [ ] Enhanced metadata extraction
- [ ] Multi-language support

---

## üìà Project Status

**Status**: Active Development üöÄ

Last Updated: January 2026

---

<div align="center">

**Star ‚≠ê this repository if you find it useful!**

[Report Bug](https://github.com/arupa444/The-Extra-Extractor-RAG/issues) ¬∑ 
[Request Feature](https://github.com/arupa444/The-Extra-Extractor-RAG/issues) ¬∑ 
[Documentation](https://github.com/arupa444/The-Extra-Extractor-RAG/wiki)

</div>
